# –†–µ–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ (Tima vs Ulug) —Å –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞.
# pip install sounddevice webrtcvad-wheels joblib torchaudio speechbrain==0.5.16 huggingface_hub

import os, sys, queue, time, shutil, json, pathlib
from pathlib import Path
import numpy as np
import sounddevice as sd
import webrtcvad
import torch, torchaudio
from joblib import load
from huggingface_hub import snapshot_download
from speechbrain.pretrained import SpeakerRecognition

# ====== –ê–Ω—Ç–∏-symlink –¥–ª—è Windows (–∫–∞–∫ –≤ —Ç–≤–æ–∏—Ö —Å–∫—Ä–∏–ø—Ç–∞—Ö) ======
_orig_symlink_to = pathlib.Path.symlink_to
def _copy_instead_of_symlink(self, target, *args, **kwargs):
    src = Path(target); dst = self
    dst.parent.mkdir(parents=True, exist_ok=True)
    try:
        if src.exists():
            if src.is_dir(): shutil.copytree(src, dst, dirs_exist_ok=True)
            else:
                if dst.exists():
                    try:
                        if dst.is_dir(): shutil.rmtree(dst)
                        else: dst.unlink()
                    except Exception: pass
                shutil.copy2(src, dst)
    except Exception:
        return
pathlib.Path.symlink_to = _copy_instead_of_symlink
os.environ["HF_HUB_DISABLE_SYMLINKS"] = "1"
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"
os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"

# ====== –ü—É—Ç–∏/–º–æ–¥–µ–ª—å ======
SID_MODEL = Path("models") / "sid_multi" / "sid_classifier.joblib"
MODEL_ID = "speechbrain/spkrec-ecapa-voxceleb"
LOCAL_DIR = Path("models") / "ecapa_local"

# ====== –ê—É–¥–∏–æ/–æ–Ω–ª–∞–π–Ω-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã ======
SR = 16000                # —á–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏
CHANNELS = 1              # –º–æ–Ω–æ
FRAME_MS = 20             # VAD —Ñ—Ä–µ–π–º (20–º—Å = 320 —Å—ç–º–ø–ª–æ–≤)
VAD_AGGR = 2              # –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ—Å—Ç—å VAD: 0..3 (–≤—ã—à–µ = —Å—Ç—Ä–æ–∂–µ –∫ –≥–æ–ª–æ—Å—É)
WINDOW_SEC = 2.0          # –¥–ª–∏–Ω–∞ –æ–∫–Ω–∞ —Ä–µ—á–∏ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
MIN_SPEECH_SEC = 1.0      # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ä–µ—á–µ–≤–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —á—Ç–æ–±—ã —Å—á–∏—Ç–∞—Ç—å –æ–∫–Ω–æ –≤–∞–ª–∏–¥–Ω—ã–º
SILENCE_HANG_SEC = 0.4    # —Å–∫–æ–ª—å–∫–æ —Ç–∏—à–∏–Ω—ã –ø–æ–¥—Ä—è–¥ —Å—á–∏—Ç–∞–µ–º –∫–æ–Ω—Ü–æ–º utterance
SMOOTH_WINDOWS = 5        # —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–∏–º N –æ–∫–Ω–∞–º (–≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ/—Å—Ä–µ–¥–Ω–µ–µ)
VOTE_FRAC = 0.6           # –¥–æ–ª—è –≥–æ–ª–æ—Å–æ–≤, —á—Ç–æ–±—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç–∫—É

DEVICE_TORCH = "cuda" if torch.cuda.is_available() else "cpu"

def load_spk_model():
    cache = Path(snapshot_download(repo_id=MODEL_ID))
    LOCAL_DIR.mkdir(parents=True, exist_ok=True)
    shutil.copytree(cache, LOCAL_DIR, dirs_exist_ok=True)
    spk = SpeakerRecognition.from_hparams(
        source=str(LOCAL_DIR), savedir=str(LOCAL_DIR),
        run_opts={"device": DEVICE_TORCH}
    )
    return spk

def embed_signal(spk: SpeakerRecognition, sig: torch.Tensor, sr: int) -> np.ndarray:
    """sig: torch.FloatTensor [1, T] @ sr (–æ–∂–∏–¥–∞–µ–º 16k)"""
    if sr != 16000:
        sig = torchaudio.functional.resample(sig, sr, 16000)
        sr = 16000
    sig = sig.to(DEVICE_TORCH)
    with torch.no_grad():
        emb = spk.encode_batch(sig)  # [1, 1, D]
    return emb.squeeze(0).squeeze(0).detach().cpu().numpy()

def main():
    if not SID_MODEL.exists():
        print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {SID_MODEL}\n–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏: python train_speaker_id.py")
        sys.exit(1)

    print(f"[DEVICE] torch={torch.__version__} | cuda={torch.version.cuda} | running on {DEVICE_TORCH}")

    spk = load_spk_model()
    clf_obj = load(SID_MODEL)
    clf = clf_obj["pipeline"]; label_map = clf_obj["label_map"]
    id2label = {int(k): v for k, v in label_map.items()}

    vad = webrtcvad.Vad(VAD_AGGR)

    frame_len = int(SR * FRAME_MS / 1000)  # 320
    window_len = int(SR * WINDOW_SEC)
    min_len = int(SR * MIN_SPEECH_SEC)
    hang_len = int(SR * SILENCE_HANG_SEC / (FRAME_MS / 1000))

    audio_q = queue.Queue()

    # ========= –∞—É–¥–∏–æ-–∫–æ–ª–ª–±–µ–∫ =========
    def sd_callback(indata, frames, time_info, status):
        if status:
            # –º–æ–∂–Ω–æ –≤—ã–≤–µ—Å—Ç–∏ warning, –Ω–æ –Ω–µ —Ä–æ–Ω—è–µ–º —Å—Ç—Ä–∏–º
            pass
        # indata shape: [frames, channels]
        mono = np.mean(indata, axis=1).astype(np.float32)
        audio_q.put(mono.copy())

    # ========= –æ—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª =========
    stream = sd.InputStream(
        channels=CHANNELS, samplerate=SR, dtype="float32",
        blocksize=frame_len, callback=sd_callback
    )
    stream.start()

    print("üéô  –†–µ–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º –∑–∞–ø—É—â–µ–Ω. –ì–æ–≤–æ—Ä–∏ –≤ –º–∏–∫—Ä–æ—Ñ–æ–Ω.  (Ctrl+C –¥–ª—è –≤—ã—Ö–æ–¥–∞)\n")

    voiced_buf = []          # —Å–ø–∏—Å–æ–∫ 20–º—Å —Ñ—Ä–µ–π–º–æ–≤ —Å —Ä–µ—á—å—é (float32)
    silence_count = 0
    last_preds = []          # –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –æ–∫–Ω–∞–º (id)
    last_probas = []         # –ø–æ—Å–ª–µ–¥–Ω–∏–µ —É—Å—Ä–µ–¥–Ω—ë–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ –æ–∫–Ω–∞–º [C]

    try:
        while True:
            # –∑–∞–±–∏—Ä–∞–µ–º 20–º—Å —Ñ—Ä–µ–π–º
            frame = audio_q.get()

            # WebRTC VAD –æ–∂–∏–¥–∞–µ—Ç int16 PCM
            pcm16 = np.clip(frame * 32768.0, -32768, 32767).astype(np.int16).tobytes()
            is_speech = vad.is_speech(pcm16, SR)

            if is_speech:
                voiced_buf.append(frame)
                silence_count = 0
            else:
                silence_count += 1

            # –µ—Å–ª–∏ –Ω–∞–∫–æ–ø–∏–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ—á–∏ ‚Äî –¥–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ ¬´–æ–∫–Ω–µ¬ª
            total_len = sum(len(f) for f in voiced_buf)
            if total_len >= min_len and total_len >= window_len:
                # —Å–æ–±–µ—Ä—ë–º —Ä–æ–≤–Ω–æ WINDOW_SEC (–ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—ç–º–ø–ª—ã)
                concat = np.concatenate(voiced_buf, axis=0)
                if len(concat) > window_len:
                    concat = concat[-window_len:]
                # —Ç–µ–Ω–∑–æ—Ä [1, T]
                sig = torch.from_numpy(concat).unsqueeze(0)
                emb = embed_signal(spk, sig, SR)
                proba = clf.predict_proba(emb.reshape(1, -1))[0]  # [C]
                pred_id = int(np.argmax(proba))
                pred_label = id2label[pred_id]

                # —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ
                last_preds.append(pred_id)
                last_probas.append(proba)
                if len(last_preds) > SMOOTH_WINDOWS:
                    last_preds = last_preds[-SMOOTH_WINDOWS:]
                    last_probas = last_probas[-SMOOTH_WINDOWS:]

                # –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –∏ —Å—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                votes = np.bincount(last_preds, minlength=len(proba))
                winner = int(np.argmax(votes))
                need = max(1, int(np.ceil(VOTE_FRAC * len(last_preds))))
                passed = votes[winner] >= need
                mean_proba = np.mean(np.vstack(last_probas), axis=0)
                mean_conf = float(mean_proba[winner])

                # –ø–µ—á–∞—Ç—å —Å—Ç–∞—Ç—É—Å–∞ (–æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞)
                msg = (f"\r‚Üí pred: {id2label[winner]:<5} | conf(avg)={mean_conf:0.3f} "
                       f"| votes={votes.tolist()} need‚â•{need}/{len(last_preds)}      ")
                sys.stdout.write(msg)
                sys.stdout.flush()

                # ¬´—Å–¥–≤–∏–≥–∞–µ–º¬ª –æ–∫–Ω–æ ‚Äî –æ—Å—Ç–∞–≤–∏–º —Ö–≤–æ—Å—Ç —Ä–µ—á–∏, —á—Ç–æ–±—ã –±—ã—Å—Ç—Ä–µ–µ –¥–∞–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ
                # –±–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π 1 —Å–µ–∫ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                keep = int(SR * 1.0)
                if len(concat) > keep:
                    tail = concat[-keep:]
                    voiced_buf = [tail]
                else:
                    voiced_buf = [concat]

            # –µ—Å–ª–∏ –¥–æ–ª–≥–æ –Ω–µ—Ç —Ä–µ—á–∏ ‚Äî —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –±—É—Ñ–µ—Ä (–∫–æ–Ω–µ—Ü utterance)
            if silence_count >= hang_len and voiced_buf:
                voiced_buf.clear()
                # —Å–±—Ä–æ—Å–∏–º –æ–∫–Ω–æ, –Ω–æ —Å–≥–ª–∞–∂–∏–≤–∞—é—â–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Å—Ç–∞–≤–∏–º ‚Äî –ø—Ä–∏—è—Ç–Ω–µ–µ UX
                sys.stdout.write("\r(—Ç–∏—à–∏–Ω–∞)".ljust(80) + "\r")
                sys.stdout.flush()

    except KeyboardInterrupt:
        print("\nüõë –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
    finally:
        stream.stop(); stream.close()

if __name__ == "__main__":
    main()
